llm-d:
  sampleApplication:
    baseConfigMapRefName: basic-gpu-preset
    enabled: true
    model:
      modelArtifactURI: hf://meta-llama/Llama-3.3-70B-Instruct
      modelName: "meta-llama/Llama-3.3-70B-Instruct-basic"
      auth:
        hfToken:
          name: llm-d-hf-token
          key: HF_TOKEN

    prefill:
      replicas: 0

    decode:
      replicas: 3
      extraArgs:
        - "--tensor-parallel-size"
        - "4"
        - "--disable-log-requests"
        - "--max-model-len"
        - "4096"
        - "--distributed-executor-backend"
        - "mp"
        - "--block-size"
        - "128"

    resources:
      limits:
        nvidia.com/gpu: 4
        rdma/ib: 1
      requests:
        cpu: "8"
        memory: 64Gi
        nvidia.com/gpu: 4
        rdma/ib: 1
  redis:
    enabled: false
  modelservice:
    rbac:
      create: false
    nodeSelector:
      compute.coreweave.com/node-pool: h100
    epp:
      defaultEnvVarsOverride:
        - name: ENABLE_KVCACHE_AWARE_SCORER
          value: "false"
        - name: ENABLE_PREFIX_AWARE_SCORER
          value: "false"
        - name: ENABLE_LOAD_AWARE_SCORER
          value: "false"
        - name: ENABLE_SESSION_AWARE_SCORER
          value: "false"
        - name: PD_ENABLED
          value: "false"
        - name: PD_PROMPT_LEN_THRESHOLD
          value: "10"
        - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
          value: "false"