llm-d:
  modelservice:
    nodeSelector:
      compute.coreweave.com/node-pool: h100

    epp:
      defaultEnvVarsOverride:
        - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
          value: "true"
        - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: DECODE_ENABLE_KVCACHE_AWARE_SCORER
          value: "true"
        - name: DECODE_ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: ENABLE_KVCACHE_AWARE_SCORER
          value: "true"
        - name: ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: PD_ENABLED
          value: "false"
        - name: PD_PROMPT_LEN_THRESHOLD
          value: "512"
      image:
        tag: v0.1.0
      metrics:
        serviceMonitor:
          interval: 10s

    inferenceSimulator:
      image:
        tag: v0.1.0

    routingProxy:
      image:
        tag: "0.0.7"

  sampleApplication:
    baseConfigMapRefName: basic-gpu-with-nixl-preset

    model:
      modelArtifactURI: hf://meta-llama/Llama-3.3-70B-Instruct
      modelName: meta-llama/Llama-3.3-70B-Instruct
      auth:
        hfToken:
          name: llm-d-hf-token
          key: HF_TOKEN

    prefill:
      replicas: 0
      extraArgs:
        - "--tensor-parallel-size"
        - "4"
        - "--disable-log-requests"
        - "--max-model-len"
        - "12288"
        - "--distributed-executor-backend"
        - "mp"
        - "--block-size"
        - "128"

    decode:
      replicas: 3
      extraArgs:
        - "--tensor-parallel-size"
        - "4"
        - "--disable-log-requests"
        - "--max-model-len"
        - "12288"
        - "--distributed-executor-backend"
        - "mp"
        - "--block-size"
        - "128"

    resources:
      limits:
        nvidia.com/gpu: 4
        rdma/ib: 1
      requests:
        cpu: "8"
        memory: 64Gi
        nvidia.com/gpu: 4
        rdma/ib: 1

  redis:
    enabled: false
    master:
      persistence:
        enabled: true
        size: 5Gi