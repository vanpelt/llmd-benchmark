llm-d:
  modelservice:
    vllm:
      logLevel: DEBUG

    nodeSelector:
      compute.coreweave.com/node-pool: h100

    epp:
      defaultEnvVarsOverride:
        - name: ENABLE_KVCACHE_AWARE_SCORER
          value: "false"
        - name: ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: ENABLE_LOAD_AWARE_SCORER
          value: "true"
        - name: ENABLE_SESSION_AWARE_SCORER
          value: "false"
        - name: PD_ENABLED
          value: "false"
        - name: PD_PROMPT_LEN_THRESHOLD
          value: "10"
        - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
          value: "false"
      image:
        tag: v0.1.0
      metrics:
        serviceMonitor:
          interval: 10s

    inferenceSimulator:
      image:
        tag: v0.1.0

    routingProxy:
      image:
        tag: "0.0.7"

  sampleApplication:
    baseConfigMapRefName: basic-gpu-preset

    model:
      modelArtifactURI: hf://RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8
      modelName: RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8
      auth:
        hfToken:
          name: llm-d-hf-token
          key: HF_TOKEN

    prefill:
      replicas: 0
      extraArgs:
        - "--tensor-parallel-size"
        - "2"
        - "--max-model-len"
        - "4096"

    decode:
      replicas: 3
      extraArgs:
        - "--tensor-parallel-size"
        - "2"
        - "--max-model-len"
        - "4096"

    resources:
      limits:
        nvidia.com/gpu: 2
        rdma/ib: 1
      requests:
        cpu: "8"
        memory: 64Gi
        nvidia.com/gpu: 2
        rdma/ib: 1

  redis:
    enabled: false
    master:
      persistence:
        enabled: true
        size: 5Gi