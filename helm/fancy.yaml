llm-d:
  modelservice:
    nodeSelector:
      compute.coreweave.com/node-pool: h100

    epp:
      defaultEnvVarsOverride:
        - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
          value: "true"
        - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: DECODE_ENABLE_KVCACHE_AWARE_SCORER
          value: "true"
        - name: DECODE_ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: ENABLE_KVCACHE_AWARE_SCORER
          value: "true"
        - name: ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: PD_ENABLED
          value: "true"
        - name: PD_PROMPT_LEN_THRESHOLD
          value: "512"
      image:
        tag: v0.1.0
      metrics:
        serviceMonitor:
          interval: 10s

    inferenceSimulator:
      image:
        tag: v0.1.0

    routingProxy:
      image:
        tag: "0.0.7"

  sampleApplication:
    baseConfigMapRefName: basic-gpu-with-nixl-preset

    model:
      modelArtifactURI: hf://Qwen/Qwen3-235B-A22B
      modelName: Qwen/Qwen3-235B-A22B
      auth:
        hfToken:
          name: llm-d-hf-token
          key: HF_TOKEN

    prefill:
      replicas: 3
      extraArgs:
        - --tensor-parallel-size
        - "1"
        - --enable-expert-parallel
        - --data-parallel-size
        - "2"
        - --pipeline-parallel-size
        - "2"

    decode:
      replicas: 2
      extraArgs:
        - --tensor-parallel-size
        - "1"
        - --enable-expert-parallel
        - --data-parallel-size
        - "2"
        - --pipeline-parallel-size
        - "2"

    resources:
      limits:
        nvidia.com/gpu: 4
        rdma/ib: 1
      requests:
        cpu: "8"
        memory: 64Gi
        nvidia.com/gpu: 4
        rdma/ib: 1

  redis:
    enabled: false
    master:
      persistence:
        enabled: true
        size: 5Gi